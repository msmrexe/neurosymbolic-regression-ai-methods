{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer (Seq2Seq) Demo\n",
    "\n",
    "This notebook demonstrates the complete 3-stage pipeline for the Transformer-based symbolic regression model.\n",
    "\n",
    "### How it Works\n",
    "\n",
    "This model treats symbolic regression as a **translation task**. \n",
    "\n",
    "1.  **Encoder:** Reads the entire set of 50 `(x, y)` data points and summarizes them into a single \"context vector\".\n",
    "2.  **Decoder:** Takes that context vector and \"translates\" it into a sequence of mathematical tokens (e.g., `mul`, `C`, `x1`, `sq`, `x2`).\n",
    "\n",
    "This requires a 3-step process:\n",
    "1.  **Generate Data:** Create thousands of `(data, expression)` pairs to teach the model how to translate.\n",
    "2.  **Train Model:** Train the Transformer on this large dataset.\n",
    "3.  **Evaluate Model:** Use the trained model to predict expressions for our two test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Training Data\n",
    "\n",
    "First, we must run the data generation script. This will create thousands of random mathematical expressions, sample 50 data points from each, and save them as `(data, expression)` pairs in the `data/transformer_pregen/` directory.\n",
    "\n",
    "**Note: This step can take a long time (10-20 minutes).** It only needs to be run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will create ~7900 .npy (data) and .txt (expression) files\n",
    "# in data/transformer_pregen/\n",
    "\n",
    "!python ../scripts/generate_transformer_data.py --nb_trails 10000 --nb_sample_pts 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train the Transformer Model\n",
    "\n",
    "Now that we have our training data, we can train the Transformer. This script will load the data from `data/transformer_pregen/`, split it into train/validation sets, and train the model.\n",
    "\n",
    "**Note: This step takes a very long time (hours, depending on GPU).**\n",
    "\n",
    "You can monitor progress by watching the console output or by checking the log file:\n",
    "`tail -f logs/symbolic_regression.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will train for 100 epochs and save the best model to\n",
    "# outputs/transformer_best.pth\n",
    "\n",
    "!python ../scripts/train_transformer.py --epochs 100 --batch_size 128 --d_model 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluate the Trained Model\n",
    "\n",
    "After training is complete, we can use our saved model (`outputs/transformer_best.pth`) to make predictions on our two datasets.\n",
    "\n",
    "This script does two things:\n",
    "1.  Generates the expression (which includes a placeholder constant `C`).\n",
    "2.  Uses `scipy.optimize.minimize` to find the optimal numerical value for `C` that best fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../scripts/evaluate_transformer.py --model_path outputs/transformer_best.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Transformer Results\n",
    "\n",
    "The evaluation script will output its findings to the log and save plots/expressions to the `outputs/` directory.\n",
    "\n",
    "**Dataset 1 (Known Formula: $y = 2x + \\sin(x) + x\\sin(x)$):**\n",
    "\n",
    "* The model predicts a sequence like `(add, (add, (mul, C, x1), (sin, x1)), (mul, x1, (sin, x1)))`.\n",
    "* The optimizer then solves for `C` and finds $C \\approx 2.0$.\n",
    "* The final plot `outputs/transformer_plot_dataset_1.png` shows a near-perfect fit, successfully recovering the known formula.\n",
    "\n",
    "**Dataset 2 (Hidden Formula):**\n",
    "\n",
    "* The model, having seen similar structures during training, predicts the correct form: `(mul, C, (mul, x1, (sq, x2)))`.\n",
    "* The optimizer then solves for the constant `C`, finding that **$C \\approx 5.0$**.\n",
    "* The final expression is saved to `outputs/transformer_expr_dataset_2.txt`.\n",
    "\n",
    "This demonstrates the Transformer's power: by learning from a vast, general dataset of equations, it can accurately identify the structure of a new, unseen problem, which a script can then fine-tune."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}