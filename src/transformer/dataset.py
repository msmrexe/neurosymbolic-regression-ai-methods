"""
Defines the PyTorch Dataset class for loading the pre-generated
symbolic regression data.
"""

import torch
from torch.utils.data import Dataset
import numpy as np
import glob
import logging
import os
from typing import Tuple

# Import vocab definitions from the central utils file
from .utils import VOCAB_MAP, PAD_TOKEN, SOS_TOKEN

logger = logging.getLogger(__name__)

class SymbolicRegressionDataset(Dataset):
    """
    PyTorch Dataset to load (data points, expression) pairs.
    
    Loads data from .npy files (inputs) and .txt files (targets)
    generated by `data_generator.py`.
    """
    
    def __init__(self, 
                 data_path: str, 
                 max_seq_len: int):
        """
        Args:
            data_path: Path to the root folder containing 'values' and 'ground_truth'.
            max_seq_len: The maximum length for target sequences (for padding).
        """
        self.max_seq_len = max_seq_len
        self.vocab_map = VOCAB_MAP
        
        # Find and sort all data/target files
        values_path = os.path.join(data_path, 'values', 'data_*.npy')
        gt_path = os.path.join(data_path, 'ground_truth', 'equation_*.txt')
        
        self.data_files = sorted(glob.glob(values_path))
        self.target_files = sorted(glob.glob(gt_path))
        
        if len(self.data_files) != len(self.target_files):
            logger.error(f"Mismatch! Found {len(self.data_files)} data files "
                         f"but {len(self.target_files)} target files in {data_path}")
            self.data_files = [] # Prevent errors
            self.target_files = []
            
        if not self.data_files:
            logger.warning(f"No data found in {data_path}. Did you run data generation?")
            
        logger.info(f"Found {len(self.data_files)} data samples.")

    def __len__(self) -> int:
        return len(self.data_files)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Fetches one sample.
        
        Returns:
            data (Tensor): Shape (nb_samples, max_nb_var, 1)
            target (Tensor): Shape (max_seq_len + 1) [for <SOS> token]
        """
        
        # 1. Load data points (.npy)
        data_path = self.data_files[idx]
        # Add feature dimension at the end: (50, 7) -> (50, 7, 1)
        data = torch.from_numpy(np.load(data_path)).unsqueeze(-1).float() 
        
        # 2. Load target expression (.txt) and tokenize
        target_path = self.target_files[idx]
        with open(target_path, 'r') as f:
            lines = [line.strip() for line in f.readlines()]

        tokenized = [SOS_TOKEN] # Start with <SOS>
        for token in lines:
            if token.startswith('C='):
                token_to_add = 'C' # Use the base 'C' token
            else:
                token_to_add = token
                
            if token_to_add in self.vocab_map:
                tokenized.append(self.vocab_map[token_to_add])
            else:
                logger.warning(f"Unknown token '{token_to_add}' in {target_path}")
                # Skip this token
        
        # 3. Create padded target tensor
        # We use max_seq_len + 1 to account for <SOS>
        target = torch.full((self.max_seq_len + 1,), PAD_TOKEN, dtype=torch.long)
        
        seq_len = min(len(tokenized), self.max_seq_len + 1)
        target[:seq_len] = torch.tensor(tokenized[:seq_len], dtype=torch.long)
        
        return data, target
